# -*- coding: utf-8 -*-
"""task2_130.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZvaUhFZR4qsLsNc7FOXbdnOHfIDXc9PV

<div class="alert alert-block alert-danger">

# FIT5196 Task 2 in Assessment 1
    
#### Student Name: Fahmid Tawsif Khan Chowdhury
#### Student ID: 34121315

#### Student Name: Chaitanya Tambolkar    
#### Student ID: 34093117

    
    


Date: 30 August 2024


Environment: Python 3.11.5

Libraries used:
* os (for interacting with the operating system)
* pandas 1.1.0 (for dataframe, installed and imported)
* multiprocessing (for performing processes on multi cores, included in Python 3.6.9 package)
* itertools (for performing operations on iterables)
* nltk 3.5 (Natural Language Toolkit, installed and imported)
* nltk.tokenize (for tokenization, installed and imported)
* nltk.stem (for stemming the tokens, installed and imported)
* sklearn.feature_extraction.text import CountVectorizer: to convert text data into a matrix of token counts.

    </div>

<div class="alert alert-block alert-info">
    
## Table of Contents

</div>

[1. Introduction](#Intro) <br>
[2. Importing Libraries](#libs) <br>
[3. Examining Input File](#examine) <br>
[4. Data Preprocessing](#load) <br>
$\;\;\;\;$[4.1. Tokenization](#tokenize) <br>
$\;\;\;\;$[4.2. Context-Independent Stopwords Removal](#whetev) <br>
$\;\;\;\;$[4.3. Context-Dependent Stopwords and Rare Tokens Removal](#dep) <br>
$\;\;\;\;$[4.4. Porter Stemming ](#stem) <br>
$\;\;\;\;$[4.5. Bigrams ](#bigram) <br>
[5. Writing Output Files](#write) <br>
$\;\;\;\;$[5.1. Vocabulary List](#write-vocab) <br>
$\;\;\;\;$[5.2. Sparse Matrix](#write-sparseMat) <br>
[6. Summary](#summary) <br>
[7. References](#Ref) <br>

<div class="alert alert-block alert-success">
    
## 1.  Introduction  <a class="anchor" name="Intro"></a>

This assessment focuses on processing and analyzing textual data extracted from user reviews. The dataset requires extensive cleaning, tokenization, and transformation into structured formats. The primary goal is to prepare the data for subsequent natural language processing tasks, such as generating vocabulary files and count vectors. The task involves leveraging Python libraries like pandas, nltk, and sklearn to execute various preprocessing steps efficiently.

<div class="alert alert-block alert-success">
    
## 2.  Importing Libraries  <a class="anchor" name="libs"></a>

The following packages were used to accomplish the related tasks:

* **os:** to interact with the operating system, e.g., navigate through folders to read files.
* **re:** to define and use regular expressions for pattern matching in text data.
* **pandas:** to work with dataframes, including data manipulation, cleaning, and analysis.
* **json:** for reading and manipulating JSON files
* **multiprocessing:** to perform processes on multiple cores for faster performance, especially when handling large datasets.
* **itertools (chain):** to flatten lists of lists into a single list, useful for aggregating data.
* **nltk:** the Natural Language Toolkit, used for various text processing tasks.
    * **nltk.probability:** to calculate and analyze the frequency distribution of words or tokens.
    * **nltk.tokenize (RegexpTokenizer):** to tokenize text based on a defined regular expression pattern.
    * **nltk.tokenize (MWETokenizer):** to tokenize multi-word expressions as single tokens.
    * **nltk.stem (PorterStemmer):** to perform stemming, which reduces words to their root form.
    * **nltk.util (ngrams):** to generate n-grams from a sequence of tokens, useful for text analysis and finding collocations.
 * **sklearn.feature_extraction.text import CountVectorizer:** to convert text data into a matrix of token counts.
"""

import os
import re
import json
import pandas as pd
import multiprocessing
from itertools import chain
import nltk
from nltk.probability import *
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import MWETokenizer
from nltk.stem import PorterStemmer
from nltk.util import ngrams
from sklearn.feature_extraction.text import CountVectorizer

"""-------------------------------------

<div class="alert alert-block alert-success">
    
## 3.  Examining Input File <a class="anchor" name="examine"></a>
"""

# from google.colab import drive
# drive.mount('/content/drive')

json_file_path = '/Users/fahmidtawsifkhanchowdhury/Documents/Monash/S3/FIT5196/Assignment 1/Submissions/task1_130.json'
csv_file_path = '/Users/fahmidtawsifkhanchowdhury/Documents/Monash/S3/FIT5196/Assignment 1/Submissions/task1_130.csv'

with open(json_file_path, 'r', encoding='utf-8') as file:
    json_data = json.load(file)

df_csv = pd.read_csv(csv_file_path, encoding='utf-8')

"""Let's examine what is the content of the file."""

df_csv.head()

"""For this purpose, we need to filter the gmap_ids of the businesses that have review_count more than 70.

In this step, the dataset is filtered to focus on businesses with more than 70 text reviews. The dataframe df_csv is filtered to extract gmap_ids from businesses meeting this criterion, which are then stored in a list called gmap_ids_review_fields. This filtered list is compared against the total list of gmap_ids in the original dataset, to uderstand how much many businesses have more than 70 reviews.
"""

# Filter the dataframe to include only rows where the number of text reviews ('review_text_count') is greater than 70
df_review_fields = df_csv[df_csv['review_text_count'] > 70]

# Extract the list of gmap_ids from the filtered dataframe
gmap_ids_review_fields = df_review_fields['gmap_id'].tolist()

# Extract the list of all gmap_ids from the original dataframe
gmap_ids_orginial = df_csv['gmap_id'].tolist()

print(gmap_ids_review_fields[:5])
print()
print("Total Gmap_id's in original csv file:",len(gmap_ids_orginial))
print("Total Gmap_id's with more than 70 text reviews:",len(gmap_ids_review_fields))

"""Two dictionaries, extracted_reviews and cleaned_reviews, are initialized to store the review texts for each business (gmap_id) that has more than 70 text reviews. The code iterates through the list of filtered gmap_ids (gmap_ids_review_fields) and, for each gmap_id, retrieves the associated reviews from the json_data. The review texts are extracted, excluding those labeled as ‘None’. These extracted reviews are stored in the extracted_reviews dictionary. To ensure the data is clean and usable for further analysis, any empty strings or None values are removed from the reviews before storing them in the cleaned_reviews dictionary."""

# Initialize dictionaries to store extracted and cleaned reviews
extracted_reviews = {}
cleaned_reviews = {}

# Iterate over the list of gmap_ids that have more than 70 text reviews
for gmap_id in gmap_ids_review_fields:
    if gmap_id in json_data:
        # Retrieve all reviews for the current gmap_id, defaulting to an empty list if none are found
        all_reviews = json_data[gmap_id].get('reviews', [])

        # Extract the review text for each review ignoring 'None'
        texts = [i.get('review_text') for i in all_reviews if i.get('review_text') != 'None']

        # Store the extracted reviews in the 'extracted_reviews' dictionary
        extracted_reviews[gmap_id] = texts

        # Further clean the reviews by removing any empty strings or None values, and store them in 'cleaned_reviews'
        cleaned_reviews[gmap_id] = [review for review in texts if review]

# Examine cleaned_reviews
# for gmap_id, reviews in list(cleaned_reviews.items())[:1]:
#     print(f"Gmap ID: {gmap_id}\nReviews: {reviews}\n")

"""<div class="alert alert-block alert-success">
    
## 4.  Data Preprocessing <a class="anchor" name="load"></a>

In this section, we perform several data preprocessing steps to create a clean and appropriate vocabulary file. The preprocessing steps carried out include the following order:

**Tokenization:**
- The text is tokenized using the regular expression "[a-zA-Z]+", which ensures that only alphabetic tokens are extracted. Non-alphabetic characters and irrelevant tokens are filtered out during this process.

**Removal of Short Tokens:**
- Tokens with a length of less than 3 characters are removed. This step eliminates excessively short tokens that are unlikely to contribute meaningful information.

**Context-Independent Stopwords Removal:**
- Context-independent stopwords are removed from the tokenized data. The list of stopwords is provided in the stopwords_en.txt file.

**Context-Dependent Stopwords Removal:**
- Context-dependent stopwords are removed based on their frequency across businesses. Tokens that appear in more than 95% of the businesses with at least 70 text reviews are considered context-dependent stopwords.

**Removal of Rare Tokens:**
- Rare tokens are removed from the vocabulary. Tokens that appear in less than 5% of the businesses with at least 70 text reviews are considered rare tokens.

**Stemming:**
- The Porter Stemmer is applied to the remaining tokens to reduce them to their root forms.

**Bigram Extraction:**
- The first 200 meaningful bigrams are included in the vocabulary using the PMI (Pointwise Mutual Information) measure.

We carefully selected and ordered these steps as they made the most sense for the vocabulary construction.

<div class="alert alert-block alert-warning">
    
### 4.1. Tokenization <a class="anchor" name="tokenize"></a>

Tokenization is a principal step in text processing and producing unigrams. In this section, we applied a regular expression tokenizer to extract alphabetic tokens. Tokens with a length of less than 3 characters were removed to filter out less meaningful words.
"""

# Tokenize and remove tokens less than length 3
tokenizer = RegexpTokenizer(r'[a-zA-Z]+')

def tokenize_text(text):
    return tokenizer.tokenize(text)

tokenized_reviews = {}

for gmap_id, reviews in cleaned_reviews.items():
    tokenized_reviews[gmap_id] = [[token for token in tokenize_text(review)
                                   if len(token) >= 3] for review in reviews]

# Check output
# tokenized_reviews

"""<div class="alert alert-block alert-warning">
    
### 4.2. Context-Independent Stopwords Removal <a class="anchor" name="whetev"></a>

In this section, we addressed context independent stopwords by utilizing a predefined list of common English stopwords. These words were loaded from an external file.
"""

# Remove context independent Stop Words
import nltk

# Path to the stopwords file
stopwords_file_path = '/Users/fahmidtawsifkhanchowdhury/Documents/Monash/S3/FIT5196/Assignment 1/stopwords_en.txt'

with open(stopwords_file_path, 'r', encoding='utf-8') as file:
    stopwords_set = set(file.read().strip().split('\n'))

# Display the number of stopwords loaded (optional)
print(f"Number of stopwords loaded: {len(stopwords_set)}")

# remove the stopwords from the tokens
filtered_tokens = {}

for gmap_id, reviews in tokenized_reviews.items():
    filtered_tokens[gmap_id] = [[token for token in review if token.lower() not in stopwords_set]
                                 for review in reviews]

# filtered_tokens

"""<div class="alert alert-block alert-warning">
    
### 4.3. Context-Dependent Stopwords and Rare Tokens Removal<a class="anchor" name="dep"></a>

In this section, we addressed context dependent stopwords as well as rare tokens.

- Context dependent stopwords are the words that appear in more than 95% of the businesses that have at least 70 text reviews.
- Rare tokens are words that appear in less than 5% of the businesses that have at least 70 text reviews.

We define thresholds to determine which words are considered context-dependent stopwords and rare tokens. The max_threshold is set to 95% of the businesses (with at least 70 text reviews) to identify words that appear too frequently across businesses, making them less informative. The min_threshold is set to 5% to filter out words that appear too infrequently.
"""

# Define the threshold levels for Context dependent stopwords and Rare tokens
max_treshold = 0.95 * int(len(gmap_ids_review_fields))
min_treshold = 0.05 * int(len(gmap_ids_review_fields))

"""For each business, the unique words across all its reviews are identified and stored as sets. These sets are then combined into a single list of words representing all businesses.

A frequency distribution (FreqDist) is created for the words, reflecting how many businesses each word appears in. This distribution helps in identifying words that are either too common (context-dependent stopwords) or too rare.
"""

# First, get the set of unique words for each business
unique_words_per_business = [set(chain.from_iterable(reviews)) for reviews in filtered_tokens.values()]

# Flatten the list of sets into a single iterable
words = list(chain.from_iterable(unique_words_per_business))

# Create the frequency distribution
frequency_words = FreqDist(words)

# Output the word frequencies, which reflect the number of businesses each word appears in
# for word, freq in frequency_words.most_common():
#     print(f"'{word}' appears in {freq} businesses")

"""Words that appear in more than 95% of the businesses are identified as high-frequency tokens, which are context-dependent stopwords. Words appearing in fewer than 5% of businesses are considered rare tokens. Both sets of tokens are combined into a single set of tokens_to_remove."""

# Identify high-frequency tokens (context-dependent stopwords)
high_freq_tokens = {token for token, freq in frequency_words.items() if freq >= max_treshold}

# Identify low-frequency tokens (rare tokens)
low_freq_tokens = {token for token, freq in frequency_words.items() if freq <= min_treshold}

# Combine the high-frequency and low-frequency tokens into a single set for removal
tokens_to_remove = high_freq_tokens.union(low_freq_tokens)

# Display the set
# print("Context Dependent Stopwords:\n", tokens_to_remove)

"""The tokens are cleaned by iterating over each review and removing any tokens identified as either context-dependent stopwords or rare tokens. This results in a refined token list that excludes both extremely common and rare words."""

cleaned_tokens = {}

for gmap_id, reviews in filtered_tokens.items():
    cleaned_tokens[gmap_id] = [[token for token in review if token.lower() not in tokens_to_remove]
                                 for review in reviews]
# cleaned_tokens

"""<div class="alert alert-block alert-warning">
    
### 4.4. Porter Stemming <a class="anchor" name="stem"></a>

The aim of this section is to apply the Porter Stemming algorithm to the tokenized review data and prepare a comprehensive list of all stemmed tokens.
"""

# create an instance of the PorterStemmer
stemmer = PorterStemmer()

"""The code iterates over the cleaned_tokens dictionary. For each gmap_id, the algorithm applies the Porter Stemmer to every token in the reviews, converting the tokens to their root forms. The stemmed tokens are then stored in a new dictionary called stemmed_tokens, where each gmap_id is associated with its corresponding list of stemmed reviews."""

# Initialize an empty dictionary to store the stemmed tokens for each gmap_id
stemmed_tokens = {}

# Iterate over the tokenized reviews (with no stopwords) for each gmap_id
for gmap_id, reviews in cleaned_tokens.items():
    # For each review, apply the Porter stemmer to each token in the review
    stemmed_tokens[gmap_id] = [
        [stemmer.stem(token) for token in review]
        for review in reviews]

# stemmed_tokens

"""All tokens from the stemmed_tokens dictionary are aggregated into a single list called all_tokens."""

all_tokens = [token for reviews in stemmed_tokens.values() for review in reviews for token in review]
len(all_tokens)

"""<div class="alert alert-block alert-warning">
    
### 4.5. Bigrams <a class="anchor" name="bigram"></a>

In this section, we extracted the first 200 meaningful bigrams that are included in the vocabulary using the PMI (Pointwise Mutual Information) measure.
"""

# Initialize the Bigram Association Measures
bigram_measures = nltk.collocations.BigramAssocMeasures()

# Find bigrams in the list of all tokens using the BigramCollocationFinder
finder = nltk.collocations.BigramCollocationFinder.from_words(all_tokens)

# Select the top 200 bigrams based on PMI (Pointwise Mutual Information)
top_200_bigrams = finder.nbest(bigram_measures.pmi, 200)

# Join the bigram pairs with an underscore to create a single token
bigrams = ['_'.join(i) for i in top_200_bigrams]

"""The vocab list is created by combining all unique unigrams (all_tokens) and the top 200 bigrams."""

vocab = sorted(set(all_tokens + bigrams))

"""The vocabulary is sorted and converted into a dictionary (vocab_dict) where each vocabulary item is assigned a unique index."""

vocab_dict = {item: index for index, item in enumerate(vocab)}

# vocab_dict

"""<div class="alert alert-block alert-success">
    
## 5. Writing Output Files <a class="anchor" name="write"></a>

files need to be generated:
* Vocabulary list
* Sparse matrix (count_vectors)

This is performed in the following sections.

<div class="alert alert-block alert-warning">
    
### 5.1. Vocabulary List <a class="anchor" name="write-vocab"></a>

This step involves saving the generated vocabulary dictionary to a specified file path for future use.
"""

# Destination file path
destination_path = '/Users/fahmidtawsifkhanchowdhury/Documents/Monash/S3/FIT5196/Assignment 1/Submissions'

# Open the file in write mode and write the vocabulary dictionary
with open(f'{destination_path}/130_vocab.txt', 'w') as f:
    for key, value in vocab_dict.items():
        f.write(f"{key}:{value}\n")

"""<div class="alert alert-block alert-warning">
    
### 5.2. Sparse Matrix <a class="anchor" name="write-sparseMat"></a>

In this step, the tokenized and stemmed reviews are first flattened into a single list of tokens for each business (gmap_id).
"""

# Initialize an empty dictionary
flattened_tokens = {}

# Iterate through the stemmed tokens for each gmap_id
for ids, reviews in stemmed_tokens.items():
    # Flatten the list of lists (reviews) into a single list of tokens for this gmap_id
    flattened_list = [token for review in reviews for token in review]
    # Store the flattened list in the dictionary
    flattened_tokens[ids] = flattened_list

"""A CountVectorizer is then initialized with the predefined vocabulary dictionary to map each token to a unique index. For each gmap_id, the flattened list of tokens is transformed into a count vector, which records the frequency of each token. These count vectors are stored in a list."""

# Initialize a CountVectorizer with the predefined vocabulary
vectorizer = CountVectorizer(vocabulary=vocab_dict)

# Initialize an empty list
countvector = []

# Iterate through the flattened tokens for each gmap_id
for ids, review in flattened_tokens.items():
    # Join the tokens into a single string of text for each gmap_id
    total_text = ' '.join(review)
    # Transform the text into a count vector using the vectorizer
    total_vectors = vectorizer.transform([total_text])
    indices = total_vectors.indices  # Indices of the tokens
    counts = total_vectors.data  # Counts of the tokens

    # Create a list of token indices and their counts in the format "index:count"
    token_indices = [
        f"{index}:{counts[i]}"
        for i, index in enumerate(indices) if counts[i] > 0
    ]
    if token_indices:
        countvector.append(f"{ids}," + ",".join(token_indices))

"""The count vector list written to a text file in a specific format that associates each gmap_id with its corresponding token counts. This output serves as a sparse representation of the reviews."""

# Write the count vectors to a text file

with open(f'{destination_path}/130_countvec.txt', 'w') as f:
    for line in countvector:
        f.write(f"{line}\n")

"""-------------------------------------

<div class="alert alert-block alert-success">
    
## 6. Summary <a class="anchor" name="summary"></a>

- In this task, we successfully extracted and processed textual data from the output files of task 1.
- The process began with loading the data, followed by tokenization and the removal of irrelevant tokens based on - length and stopword criteria.
- Further refinement involved the removal of context-dependent stopwords and rare tokens, ensuring that the vocabulary was both meaningful and representative of the data.
- The data was then stemmed using the Porter Stemmer.
- Significant bigrams were identified and included in the final vocabulary.
- The clean and structured data was exported into a `.txt` format, and count vectors were generated and exported in `.txt` format as well.

-------------------------------------

<div class="alert alert-block alert-success">
    
## 7. References <a class="anchor" name="Ref"></a>
    
</div>
    
[1] Python Software Foundation. (2024). nltk Documentation. Available at: https://www.nltk.org/ Accessed on: 22/08/2024.

[2] Python Software Foundation. (2024). sklearn.feature_extraction.text.CountVectorizer — Convert a collection of text documents to a matrix of token counts. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html Accessed on: 30/08/2024.

[3] Stack Overflow. (2023). How to flatten a list of lists in Python? Available at: https://stackoverflow.com/questions/952914 Accessed on: 22/08/2024.

[4] Stack Overflow. (2022). How to remove stop words using nltk or python? Available at: https://stackoverflow.com/questions/5486337 Accessed on: 24/08/2024.


[5] Python Software Foundation. (2024). itertools.chain — Combine multiple iterables into a single iterable. Available at: https://docs.python.org/3/library/itertools.html#itertools.chain Accessed on: 24/08/2024.

[6] Python Software Foundation. (2024). nltk.tokenize.RegexpTokenizer — Tokenize text using a regular expression. Available at: https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.RegexpTokenizer Accessed on: 26/08/2024.

[7] Python Software Foundation. (2024). nltk.stem.PorterStemmer — Reduce words to their root form. Available at: https://www.nltk.org/_modules/nltk/stem/porter.html Accessed on: 26/08/2024.

[8] Week_5_applied_session-2 (2024) https://learning.monash.edu/mod/resource/view.php?id=3049671. Accessed on: 26/08/2024.

## --------------------------------------------------------------------------------------------------------------------------
"""