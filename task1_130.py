# -*- coding: utf-8 -*-
"""task1_130.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1htLQ-UEBjAr7m3-C7AKGTnG1XEOKjSm8

<div class="alert alert-block alert-success">
    
# FIT5196 Task 1 in Assessment 1
    
#### Student Name: Fahmid Tawsif Khan Chowdhury
#### Student ID: 34121315

#### Student Name: Chaitanya Tambolkar    
#### Student ID: 34093117

    
    


Date: 30 August 2024


Environment: Python 3.11.5

Libraries used:
* re (for regular expression, installed and imported)
* pandas (for data manipulation)
* os (for interacting with the operating system)
* json (for reading and manipulating JSON files)
* datetime (for handling and manipulating date and time data)
    
</div>

<div class="alert alert-block alert-danger">
    
## Table of Contents

</div>    

[1. Introduction](#Intro) <br>
[2. Importing Libraries](#libs) <br>
[3. Examining Patent Files](#examine) <br>
$\;\;\;\;$[3.1. Examine the `.xlsx` file](#xlsx) <br>
$\;\;\;\;$[3.2. Examine the `.txt` files](#txt) <br>
$\;\;\;\;$[3.3. Observations from the Raw Data](#obs) <br>
[4. Data Parsing and Preprocessing](#load) <br>
$\;\;\;\;$[4.1. Parsing and Preprocessing - `.xlsx`](#parse_xlsx) <br>
$\;\;\;\;$[4.2. Parsing and Preprocessing - `.txt`](#parse_txt) <br>
$\;\;\;\;$[4.3. Defining Regular Expressions](#Reg_Exp) <br>
$\;\;\;\;$[4.4. Data Merging, Cleaning and Column Consolidation](#Read) <br>
$\;\;\;\;$[4.5. Concatenate Both Dataframes](#latin) <br>
[5. Creating the Output Files](#main_file) <br>
$\;\;\;\;$[5.1. Create the JSON File](#json_file) <br>
$\;\;\;\;$[5.2.  Create the CSV File](#csv_file) <br>
[6. Summary](#summary) <br>
[7. References](#Ref) <br>

-------------------------------------

<div class="alert alert-block alert-warning">

## 1.  Introduction  <a class="anchor" name="Intro"></a>
    
</div>

This assessment regards extracting data from semi-sctuctured text files. The dataset contained 14 `.txt` files and 1 `.xlsx` file, which included various information about user reviews. In particular, the files include details such as user IDs, review text, their timestamps, the ratings provided, images, and responses by businesses. The data is distributed across multiple sheets in the `.xlsx` file. The `.txt` provide the same information but in an `html` format.

The primary task is to process and consolidate this information to create a structured dataset that can be used for further analysis. This involves reading and parsing the files, handling missing values, normalising the data formats, and merging the datasets to create a comprehensive view of all the data. The extracted and processed data will then be used to perform exploratory data analysis (EDA) and prepare the data for more advanced analytical tasks.

-------------------------------------

<div class="alert alert-block alert-warning">
    
## 2.  Importing Libraries  <a class="anchor" name="libs"></a>
 </div>

The packages to be used in this assessment are imported in the following. They are used to fulfill the following tasks:

* **re:** to define and use regular expressions
* **pandas:** for data manipulation
* **os:** for interacting with the operating system
* **json:** for reading and manipulating JSON files
* **datetime:** for handling and manipulating date and time data
"""

import re
import pandas as pd
import os
import json
from datetime import datetime

# from google.colab import drive
# drive.mount('/content/drive')

"""-------------------------------------

<div class="alert alert-block alert-warning">

## 3.  Examining Raw Data <a class="anchor" name="examine"></a>

 </div>

First of all, we need to examine the contents of the raw data. To do this, we start by reading the `.xlsx` and `.txt` files.

The `.xlsx` file contains multiple sheets, so we load all relevant sheets (`Sheet0` through `Sheet15`) into a dictionary where each sheet name is a key, and the corresponding data is a value. After loading the data, we check the names of the sheets to ensure we have correctly loaded them. Finally, we inspect the first few rows of `Sheet3` to get an initial understanding of its contents.

<div class="alert alert-block alert-info">
    
### 3.1. Examine the `.xlsx` file <a class="anchor" name="xlsx"></a>
"""

xlsx_file_path = "/Users/fahmidtawsifkhanchowdhury/Documents/Monash/S3/FIT5196/Assignment 1/student_group130/group130.xlsx"

# Read the xlsx file
data_xlsx = pd.read_excel(f"{xlsx_file_path}",
                     sheet_name=['Sheet0', 'Sheet1', 'Sheet2', 'Sheet3','Sheet4', 'Sheet5','Sheet6', 'Sheet7','Sheet8', 'Sheet9','Sheet10', 'Sheet11', 'Sheet12', 'Sheet13','Sheet14', 'Sheet15'], )
# Check the sheet names
data_xlsx.keys()

# Check Sheet 3
data_xlsx["Sheet3"].head()

"""<div class="alert alert-block alert-info">
    
### 3.2. Examine the `.txt` files <a class="anchor" name="txt"></a>
"""

# Load all txt file and read them

txt_files_path = "/Users/fahmidtawsifkhanchowdhury/Documents/Monash/S3/FIT5196/Assignment 1/student_group130/"

file_paths = [
    f"{txt_files_path}group130_0.txt",
    f"{txt_files_path}group130_1.txt",
    f"{txt_files_path}group130_2.txt",
    f"{txt_files_path}group130_3.txt",
    f"{txt_files_path}group130_4.txt",
    f"{txt_files_path}group130_5.txt",
    f"{txt_files_path}group130_6.txt",
    f"{txt_files_path}group130_7.txt",
    f"{txt_files_path}group130_8.txt",
    f"{txt_files_path}group130_9.txt",
    f"{txt_files_path}group130_10.txt",
    f"{txt_files_path}group130_11.txt",
    f"{txt_files_path}group130_12.txt",
    f"{txt_files_path}group130_13.txt",
    f"{txt_files_path}group130_14.txt"
]

all_files = []

for i in file_paths:
    with open(i, 'r', encoding="utf-8") as file:
        content = file.read()
        all_files.append(content)

text = ''.join(all_files)

"""<div class="alert alert-block alert-info">
    
### 3.3. Observations from the Raw Data <a class="anchor" name="obs"></a>

Having examined the file content, the following observations were made:

The `.xlsx` has several issues with its structure.
* All the sheets have several rows with missing values, suggesting empty rows.
* Similarly, they have meaningless empty columns. Both these empty rows and column need to be removed.
* `time` column seems to suggest a unix format, however, its data type is float whereas it needs to be an integer.
    
    
    

The data in the `.txt` files are structured in an XML format, which includes open and closing tags. Several issues were observed in the format of the tags:
* Different variations were observed in the naming of same data field. Some had '_' and '.'. For example, User ID: `<user_id>`, `<UserId.>`, `<user>`, `<userid.>`
* Some tags are incorrectly closed. For example, two '//' present (for example, `<//Time>`) instead of one '/' (for example, `</Time>`).
* Issues with the capitalisation, symbols and spacing within the tags. For example, `< GmapID>`, `< gmapid>`, `<gmapID>`, `<gmap_id>`.
* Some tags have represent the same data field but are named completely different. For example, `<review>` and `<text>`. Both imply the review written by the user.

<div class="alert alert-block alert-warning">

## 4.  Data Parsing and Preprocessing <a class="anchor" name="load"></a>

</div>

<div class="alert alert-block alert-info">
    
### 4.1. Parsing and Preprocessing - `.xlsx`<a class="anchor" name="parse_xlsx"></a>

In this section, the files are parsed and processed to prepare the dataset for further analysis. Therelevant columns (user_id, name, time, rating, text, pics, resp, gmap_id) are extracted from each sheet in the `.xlsx`. These sheets are then concatenated into a single DataFrame, combining all the review data into one structure. The DataFrame is examined for missing values and data types. Empty strings are replaced with NaN values to standardise the missing data representation. The time column, containing Unix timestamps, is converted from float to integer and the missing timestamps are set to NaN. This preprocessing step ensures that the data is clean and structured.
"""

# Extract the column anmes from the excel file
listofsheets = []

for i in data_xlsx.values():
    df = i[["user_id","name", "time" , "rating","text","pics","resp","gmap_id"]]
    listofsheets.append(df)

print(len(listofsheets))

# Concatenate the sheets into one dataframe
df_concat = pd.concat(listofsheets)
df_concat

# Check the number of null values per column
df_concat.isna().sum()

# Check data types
df_concat.dtypes

# Replace empty stings to NA
df_concat['rating'] = df_concat['rating'].replace('', pd.NA)
df_concat['time'] = df_concat['time'].replace('', pd.NA)
df_concat['time'] = df_concat['time'].fillna(0)

# Convert time to integer
df_concat['time'] = df_concat['time'].apply(lambda x: int(x) if pd.notnull(x) else x)
df_concat['time'] = df_concat['time'].apply(lambda x: pd.NA if x==0 else x)

df_concat.head(10)

"""<div class="alert alert-block alert-info">
    
### 4.2. Parsing and Preprocessing - `.txt`<a class="anchor" name="parse_txt"></a>

In this section, we perform a series of string replacements to clean up and format the content. The operations include removing unnecessary spaces after angle brackets, replacing double slashes with single slashes, removing newline characters, and eliminating specific XML tags such as `<dataset>` and `</dataset>`. Additionally, occurrences of the word “None” are removed, and the XML version declaration (?xml version="1.0" encoding="UTF-8"?>) is stripped out.
"""

# Perform a series of text replacements to clean up the content
text_replace = text.replace("< ", "<")
text_replace = text_replace.replace("//", "/")
text_replace = text_replace.replace("\n", "")
text_replace = text_replace.replace("<dataset>", "")
text_replace = text_replace.replace("</dataset>", "")
text_replace = text_replace.replace("None", "")

# Remove the XML version declaration
text_replace = text_replace.replace("""?xml version="1.0" encoding="UTF-8"?>""", "")

# text_replace

# Check total number of records
print(text_replace.count("<record>"))
print(text_replace.count("</record>"))

"""-------------------------------------

<div class="alert alert-block alert-info">
    
### 4.3. Defining Regular Expressions <a class="anchor" name="Reg_Exp"></a>

Defining correct regular expressions is crucial in extracting desired information from the text efficiently.  In the code snippet provided, a regular expression pattern `(pattern = r'<([^>]+)>(.*?)<\/\1>')` is used to match and capture XML-like tags and their corresponding content. This pattern identifies a pair of matching start and end tags, along with the content between them.

The re.findall function is used to search for all occurrences of records enclosed within `<record>` tags in the text_replace. Each record is then processed using the previously defined pattern to extract key-value pairs, which are stored as dictionaries. These dictionaries are collected into a list named data.

The list of dictionaries is converted into a pandas DataFrame (df), where each key becomes a column, and each dictionary corresponds to a row.
"""

pattern = r'<([^>]+)>(.*?)<\/\1>'

records = re.findall(r'<record>(.*?)<\/record>', text_replace)

data = []

for r in records:
    all_records = dict(re.findall(pattern, r))
    data.append(all_records)

df_text = pd.DataFrame(data)

df_text.head()

df_text.shape

"""-------------------------------------

<div class="alert alert-block alert-info">
    
### 4.4. Data Merging, Cleaning and Column Consolidation <a class="anchor" name="Read"></a>

To handle inconsistencies in column names across different datasets, we implemented a systematic approach to merge and consolidate these columns into unified attributes. We started by selecting relevant columns that represent the same data but are labeled differently across the dataset, such as variations of “user_name,” “gmap_id,” “time,” “rating,” etc. For each of these grouped columns, we created a new column (e.g., name_merged, gmap_id_merged) to store the consolidated information. A for-loop was used to iterate through each row, checking the columns in a prioritised order.

Then we focused on organising the dataset by first arranging the columns in the desired order to align with the output files. We then addressed any empty strings within the dataset, replacing them with pd.NA. Moreover, to ensure that the dataset did not contain non-informative whitespace, we checked for columns that contained only whitespace values, which could potentially skew analysis if left unaddressed. Finally, we assigned appropriate data types to the time_merged and rating_merged columns, converting them to integer and float types, respectively.
"""

df_text.columns

# Create dataframe using username attributes
df1 = df_text[["user_name", "name", "Name", "username"]]

# Create new column
df_text['name_merged'] = ""

# Create for loop to incorporate values into 'name_merged'
for index, col in df1.iterrows():

    # Use if statements to fill up 'name_merged' from "user_name", "name", "Name", "username"
    if pd.isnull(col['user_name']) != True:
        df_text.iloc[index, -1] = df1.iloc[index, 0]
    elif pd.isnull(col['name']) != True:
        df_text.iloc[index, -1] = df1.iloc[index, 1]
    elif pd.isnull(col['Name']) != True:
        df_text.iloc[index, -1] = df1.iloc[index, 2]
    elif pd.isnull(col['username']) != True:
        df_text.iloc[index, -1] = df1.iloc[index, 3]

# Drop redundant columns after merging
df_text.drop(["user_name", "name", "Name", "username"], axis=1, inplace=True)

df2 = df_text[["Gmap_id", "GmapID", "gmapID", "gmap_id"]]

df_text['gmap_id_merged'] = ""

for index, col in df2.iterrows():
    if pd.isnull(col['Gmap_id']) != True:
        df_text.iloc[index, -1] = df2.iloc[index, 0]
    elif pd.isnull(col['GmapID']) != True:
        df_text.iloc[index, -1] = df2.iloc[index, 1]
    elif pd.isnull(col['gmapID']) != True:
        df_text.iloc[index, -1] = df2.iloc[index, 2]
    elif pd.isnull(col['gmap_id']) != True:
        df_text.iloc[index, -1] = df2.iloc[index, 3]

df_text.drop( ["Gmap_id", "GmapID", "gmapID", "gmap_id"] , inplace=True, axis=1)

df3 = df_text[["user", "user_id", "userid", "UserId."]]

df_text['user_id_merged'] = ""

for index, col in df3.iterrows():
    if pd.isnull(col['user']) != True:
        df_text.iloc[index, -1] = df3.iloc[index, 0]
    elif pd.isnull(col['user_id']) != True:
        df_text.iloc[index, -1] = df3.iloc[index, 1]
    elif pd.isnull(col['userid']) != True:
        df_text.iloc[index, -1] = df3.iloc[index, 2]
    elif pd.isnull(col['UserId.']) != True:
        df_text.iloc[index, -1] = df3.iloc[index, 3]

df_text.drop( ["user", "user_id", "userid", "UserId."] , inplace=True, axis=1)

df4 = df_text[["Time", "date", "time", "Date"]]

df_text['time_merged'] = ""

for index, col in df4.iterrows():
    if pd.isnull(col['Time']) != True:
        df_text.iloc[index, -1] = df4.iloc[index, 0]
    elif pd.isnull(col['date']) != True:
        df_text.iloc[index, -1] = df4.iloc[index, 1]
    elif pd.isnull(col['time']) != True:
        df_text.iloc[index, -1] = df4.iloc[index, 2]
    elif pd.isnull(col['Date']) != True:
        df_text.iloc[index, -1] = df4.iloc[index, 3]

df_text.drop( ["Time", "date", "time", "Date"] , inplace=True, axis=1)

df5 = df_text[["rate", "Rating", "rating", "Rate"]]

df_text['rating_merged'] = ""

for index, col in df5.iterrows():
    if pd.isnull(col['rate']) != True:
        df_text.iloc[index, -1] = df5.iloc[index, 0]
    elif pd.isnull(col['Rating']) != True:
        df_text.iloc[index, -1] = df5.iloc[index, 1]
    elif pd.isnull(col['rating']) != True:
        df_text.iloc[index, -1] = df5.iloc[index, 2]
    elif pd.isnull(col['Rate']) != True:
        df_text.iloc[index, -1] = df5.iloc[index, 3]

df_text.drop( ["rate", "Rating", "rating", "Rate"] , inplace=True, axis=1)

df6 = df_text[["review", "Review", "text", "Text"]]

df_text['text_merged'] = ""

for index, col in df6.iterrows():
    if pd.isnull(col['review']) != True:
        df_text.iloc[index, -1] = df6.iloc[index, 0]
    elif pd.isnull(col['Review']) != True:
        df_text.iloc[index, -1] = df6.iloc[index, 1]
    elif pd.isnull(col['text']) != True:
        df_text.iloc[index, -1] = df6.iloc[index, 2]
    elif pd.isnull(col['Text']) != True:
        df_text.iloc[index, -1] = df6.iloc[index, 3]

df_text.drop( ["review", "Review", "text", "Text"] , inplace=True, axis=1)

df7 = df_text[["pics", "Pics", "pictures", "Pictures"]]

df_text['pics_merged'] = ""

for index, col in df7.iterrows():
    if pd.isnull(col['pics']) != True:
        df_text.iloc[index, -1] = df7.iloc[index, 0]
    elif pd.isnull(col['Pics']) != True:
        df_text.iloc[index, -1] = df7.iloc[index, 1]
    elif pd.isnull(col['pictures']) != True:
        df_text.iloc[index, -1] = df7.iloc[index, 2]
    elif pd.isnull(col['Pictures']) != True:
        df_text.iloc[index, -1] = df7.iloc[index, 3]

df_text.drop( ["pics", "Pics", "pictures", "Pictures"] , inplace=True, axis=1)

df8 = df_text[["Resp", "response", "resp", "Response"]]

df_text['resp_merged'] = ""

for index, col in df8.iterrows():
    if pd.isnull(col['Resp']) != True:
        df_text.iloc[index, -1] = df8.iloc[index, 0]
    elif pd.isnull(col['response']) != True:
        df_text.iloc[index, -1] = df8.iloc[index, 1]
    elif pd.isnull(col['resp']) != True:
        df_text.iloc[index, -1] = df8.iloc[index, 2]
    elif pd.isnull(col['Response']) != True:
        df_text.iloc[index, -1] = df8.iloc[index, 3]

df_text.drop( ["Resp", "response", "resp", "Response"] , inplace=True, axis=1)

df_text.head()

df_text.shape

# Arrange the columns as per the excel file
df_text = df_text.reindex(columns=['user_id_merged', 'name_merged', 'time_merged', 'rating_merged',
       'text_merged', 'pics_merged', 'resp_merged', 'gmap_id_merged'])
df_text.head()

# Replace empty string to NA
df_text['user_id_merged'] = df_text['user_id_merged'].replace('', pd.NA)
df_text['name_merged'] = df_text['name_merged'].replace('', pd.NA)
df_text['time_merged'] = df_text['time_merged'].replace('', pd.NA)
df_text['rating_merged'] = df_text['rating_merged'].replace('', pd.NA)
df_text['text_merged'] = df_text['text_merged'].replace('', pd.NA)
df_text['pics_merged'] = df_text['pics_merged'].replace('', pd.NA)
df_text['resp_merged'] = df_text['resp_merged'].replace('', pd.NA)
df_text['gmap_id_merged'] = df_text['gmap_id_merged'].replace('', pd.NA)

# Check for Columns with Only Whitespace Values

columns_to_clean = [
    'user_id_merged', 'name_merged', 'time_merged', 'rating_merged',
    'text_merged', 'pics_merged', 'resp_merged', 'gmap_id_merged'
]

# Regex to check strings with only whitespaces
whitespace_counts = df_text[columns_to_clean].apply(lambda col: col.str.contains(r'^\s*$', na=False).sum())

print("Whitespace-only count:")
print(whitespace_counts)

# Check for null values
df_text.isnull().sum()

# Assign appropraite data type
df_text['time_merged'] = df_text['time_merged'].apply(lambda x: int(x) if pd.notnull(x) else x)
df_text['rating_merged'] = df_text['rating_merged'].apply(lambda x: float(x) if pd.notnull(x) else x)

"""-------------------------------------

<div class="alert alert-block alert-info">
    
### 4.5. Concatenate Both Dataframes <a class="anchor" name="latin"></a>

Now that both dataframes were structured appropriately, the next step involved merging them into a single comprehensive dataset.
- After the merge, duplicates were checked and handled.
- Non-English reviews were filtered out.
- A regex pattern was used to remove any emojis.
- Unix timestamps were converted to the appropriate datetime format.
- Additionally, new columns, if_pic and if_response, were created to indicate whether pictures or responses were provided, with values of “Y” or “N.”
- A column for picture dimensions was also created to capture relevant image data.
- earliest_review_date and latest_review_date columns were also created to indicate the earliest review date and latest review date for a given business, respectively.
"""

# Change to columns to appropriate names
df_text.columns = ['user_id', 'name', 'time', 'rating','text', 'pics', 'resp', 'gmap_id']

# Analyse the shape of the two dataframes
print(df_text.shape)
print(df_concat.shape)

# Create a list of the two dataframes

df_list = []

df_list.append(df_text)
df_list.append(df_concat)

# Concatenate the two dataframes
df = pd.concat(df_list, ignore_index=True)

df.shape

df.head()

# Check for any duplications
df.duplicated().sum()

# Drop duplicates
df.drop_duplicates(inplace=True)

# Check duplicates
df.duplicated().sum()

# Reset the index
df.reset_index(drop=True, inplace=True)

# Check nulls
df.isnull().sum()

# Locate the null entry
null_name_row = df[pd.isna(df['name'])]
print(null_name_row)

# Drop the row
indices_to_drop = df[pd.isna(df['name'])].index
df = df.drop(indices_to_drop)
df.isnull().sum()

"""The steps below involve cleaning and standardizing the text column by focusing on removing non-English content and ensuring consistency in the data. Initially, we identified entries containing the phrase “(Translated by Google)” to filter out non-English text, as this phrase typically indicates automatic translation. By examining specific instances of this phrase, we observed its pattern within the text, which allowed us to construct an effective regex pattern to remove it. Subsequently, we converted the entire text column to lowercase for uniformity and used additional regex patterns to eliminate non-English content and remove any remnants of the translation indicators. This process ensures that only English text remains in a standardized format."""

# Sample lines in the text column that have "(Translated by Google)"
df.reset_index()
contains_translated = df['text'].str.contains(r'\(Translated by Google\)', na=False)
df[contains_translated].head(3)

# Check the lines to study the pattern
print(df.iloc[289, 4])
print()
print(df.iloc[737, 4])
print()
print(df.iloc[738, 4])

# Convert text column to lower case
df['text'] = df['text'].str.lower()

# Replace (translated by google) with ""
df['text'] = df['text'].str.replace(r'\(translated by google\) ', '', regex=True)

# Remove all languages other than english by using regex r'\(original\).*'
df['text'] = df['text'].str.replace(r'\(original\).*', '', regex=True)

print(df.iloc[289, 4])
print()
print(df.iloc[737, 4])
print()
print(df.iloc[738, 4])

emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"
        u"\U0001F300-\U0001F5FF"
        u"\U0001F680-\U0001F6FF"
        u"\U0001F1E0-\U0001F1FF"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251""]+", re.UNICODE)

# Sample a review with emoji
df.iloc[18, 4]

df['text'] = df['text'].apply(lambda x: emoji_pattern.sub(r'', x) if isinstance(x, str) else x)

df.iloc[18, 4]

"""In this step, we removed any emojis. We defined a regular expression pattern (emoji_pattern) that matches a wide range of emojis, including facial expressions, objects, symbols, and country flags. Using this pattern, we applied a lambda function to the text column, which substitutes any matched emojis with an empty string, effectively removing them from the text."""

# Convert unix timestamp to datetime
df['time'] = pd.to_datetime(df['time'], unit='ms', utc=True)
df['time'] = pd.to_datetime(df['time']).dt.strftime('%Y-%m-%d %H:%M:%S')
df.head(1)

# Create if_pic column to indicate presence or absence of data in pics
df['if_pic'] = df['pics'].apply(lambda x: 'Y' if pd.notna(x) and len(str(x)) > 0 else 'N')
df.head(1)

# Replace NAN to None in text column
df['text'] = df['text'].apply(lambda x: 'None' if pd.isna(x) else x)
df.tail()

# Creating a function to extract the width and height of the picture from the url
def pics_url(pics):
    if pd.isna(pics):
        return []
    dimensions = re.findall(r'w(\d+)-h(\d+)', pics)
    list_dim = [[height, width] for width, height in dimensions]
    return list_dim if list_dim else []
df['pic_dim'] = df['pics'].apply(pics_url)

print(df['pic_dim'][0])

# Create if_response column to indicate presence or absence of data in pics
df['if_response'] = df['resp'].apply(lambda x: 'Y' if pd.notna(x) and len(str(x).strip()) > 0 else 'N')
df.head(1)

# Creating the earliest review and latest review date columns
first_review = df.groupby('gmap_id')['time'].min().reset_index()
last_review = df.groupby('gmap_id')['time'].max().reset_index()
first_review.head(1)

first_review = first_review.rename(columns={'time': 'first_review_date'})
last_review = last_review.rename(columns={'time': 'last_review_date'})

df = pd.merge(df, first_review, on='gmap_id', how='left')
df = pd.merge(df, last_review, on='gmap_id', how='left')
df.head()

"""<div class="alert alert-block alert-warning">

## 5.  Creating the Output Files <a class="anchor" name="main_file"></a>

</div>

<div class="alert alert-block alert-info">
    
### 5.1. Create the JSON File <a class="anchor" name="json_file"></a>
"""

# Creating the json file by grouping on the gmap_id and assigning each of the appropriate variables¶
json_data = {}

json_group = df.groupby('gmap_id')

for gmap_id, group in json_group:
    all_data = {
        "reviews": [],
        "earliest_review_date": group['first_review_date'].iloc[0],
        "latest_review_date": group['last_review_date'].iloc[0]
    }

    for _, row in group.iterrows():
        review = {
            "user_id": row['user_id'],
            "time": row['time'],
            "review_rating": row['rating'],
            "review_text": row['text'],
            "if_pic": row['if_pic'],
            "pic_dim": row['pic_dim'],
            "if_response": row['if_response']
        }
        all_data["reviews"].append(review)

    json_data[gmap_id] = all_data

first_key = list(json_data.keys())[0]
first_value = json_data[first_key]

print(f"First key: {first_key}")
print(f"First value: {first_value}")

# Output JSON file
destination_path = '/Users/fahmidtawsifkhanchowdhury/Documents/Monash/S3/FIT5196/Assignment 1/Submissions'

json_output = f'{destination_path}/task1_130.json'
with open(json_output, 'w', encoding='utf-8') as json_file:
    json.dump(json_data, json_file, ensure_ascii=False, indent=2)

"""<div class="alert alert-block alert-info">
    
### 5.2. Create the CSV File <a class="anchor" name="csv_file"></a>
"""

# Creating the dataframe for the csv file by grouping the gmap_id and considering other columns
df_csv_output = df.copy()

df_csv_output['text'] = df_csv_output['text'].apply(lambda x: pd.NA if x == 'None' else x)

df_grouped = df_csv_output.groupby('gmap_id').agg(review_count=("rating", "count"),
                                       review_text_count=("text", "count"),
                                       response_count=("resp", "count")).reset_index()
df_grouped.iloc[0:20, : ]

print("Count of text Non-null values:",df_csv_output['text'].count())
print("Count of Text with null values:",len(df_csv_output['text']),"\n")

print("Count of review text total count non-null values:",df_grouped['review_text_count'].sum())
print("Count of review text total count with null values:",df_grouped['review_count'].sum(),"\n")

print("Count of response non-null values:",df_csv_output['resp'].count())
print("Count of response with null values:",len(df_csv_output['resp']),"\n")

print("Number of null values in responses:",df_csv_output['resp'].isna().sum())
print("Number of null values in review text:",df_csv_output['text'].isna().sum())
print("Number of null values in gmap_id:",df_csv_output['gmap_id'].isnull().sum())

# Output csv file
csv_output = f'{destination_path}/task1_130.csv'
df_grouped.to_csv(csv_output, index=False, encoding='utf-8')

"""-------------------------------------

<div class="alert alert-block alert-warning">

## 6. Summary <a class="anchor" name="summary"></a>

</div>

**Data Loading and Initial Parsing**

- The task begins with loading and parsing multiple text and Excel files. Regular expressions was used extensively to extract the necessary information from semi-structured text data. Excel sheets wereread using pandas, and the relevant data columns were extracted.


**Data Cleaning and Preprocessing**


- Empty strings in various columns were replaced with pd.NA to ensure consistent handling of missing data.
- The replace method was used to clean up unwanted strings, such as removing XML tags and non-informative text like “None”.
- Several columns with similar or duplicate information were merged. For instance, variations of the name, gmap_id, user_id, time, rating, text, pics, and resp columns were consolidated into singular columns using loops and conditional checks.
- The time column, originally in Unix timestamp format, was converted into human-readable datetime format.
- Non-English reviews (identified by the presence of the phrase “Translated by Google”) were removed to focus the analysis on English content.

**Emoji and Special Character Removal**

- A regex pattern was used to strip out emojis and special characters from the text column.

**Aggregation of Data for CSV**

- Data was grouped by gmap_id to aggregate review-related metrics.
- Review Count: The total number of reviews for each gmap_id.
- Review Text Count: The number of reviews containing text.
- Response Count: The number of reviews that received a response.


**JSON File Creation**

- The cleaned and processed data was then structured into a JSON format. Each gmap_id key holds a dictionary containing review details, including the earliest and latest review dates, and lists of individual review information.

**Export of Output Files**

- The final steps involve checking for any remaining inconsistencies, such as columns containing only whitespace, and ensuring the correct data types are set.
- The processed data has been exported to the necessary formats (e.g., CSV, JSON) for subsequent analysis.

-------------------------------------

<div class="alert alert-block alert-warning">

## 7. References <a class="anchor" name="Ref"></a>

</div>

[1] Python Software Foundation. (2024). os — Miscellaneous operating system interfaces. Available at: https://docs.python.org/3/library/os.html Accessed on: 15/08/2024.

[2] Python Software Foundation. (2024). pandas.read_excel — Reading Excel files. Available at: https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html Accessed on: 19/08/2024.

[3] Python Software Foundation. (2024). pandas.concat — Concatenate pandas objects along a particular axis. Available at: https://pandas.pydata.org/docs/reference/api/pandas.concat.html Accessed on: 19/08/2024.

[4] Python Software Foundation. (2024). re.findall — Find all matches of a pattern. Available at: https://docs.python.org/3/library/re.html#re.findall Accessed on: 19/08/2024.

[5] Python Software Foundation. (2024). pandas.DataFrame.apply — Apply a function along an axis of the DataFrame. Available at: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html Accessed on: 30/08/2024.

[6] Python Software Foundation. (2024). pandas.to_datetime — Convert argument to datetime. Available at: https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html Accessed on: 19/08/2024.

[7] Python Software Foundation. (2024). pandas.DataFrame.groupby — Group DataFrame using a mapper or by a Series of columns. Available at: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html Accessed on: 19/08/2024.

[8] Python Software Foundation. (2024). json.dump — Serialize obj as a JSON formatted stream. Available at: https://docs.python.org/3/library/json.html#json.dump Accessed on: 19/08/2024.

[9] Python Software Foundation. (2024). pandas.DataFrame.to_csv — Write DataFrame to a comma-separated values (csv) file. Available at: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html Accessed on: 19/08/2024.

[10] Week_3_applied_session: Regular Expression https://learning.monash.edu/mod/resource/view.php?id=2991871  Accessed on: 18/08/2024.

[11] Week_4_applied_session:Data Parsing https://learning.monash.edu/mod/resource/view.php?id=2838622  Accessed on: 18/08/2024.

## --------------------------------------------------------------------------------------------------------------------------
"""